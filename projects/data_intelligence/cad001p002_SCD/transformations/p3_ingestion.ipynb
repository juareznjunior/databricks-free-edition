{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23c7dca-1d37-4bed-b18a-4045429d99ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b154b7f-9534-45ea-977e-16914c969cf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definitions"
    }
   },
   "outputs": [],
   "source": [
    "table_cad001p002_bronze  = \"data_intelligence.bronze.cad001p002\"\n",
    "table_cad001p002_silver  = \"data_intelligence.silver.cad001p002\"\n",
    "table_cad001p002_control = \"data_intelligence.silver.cad001p002_control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd39fbe-a899-423c-9095-a9f5ed17ea2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_cad_date_control():\n",
    "    return spark.sql(f\"SELECT MAX(cad_date) AS last_cad_date_control FROM {table_cad001p002_control}\").first()['last_cad_date_control']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b155d0c-0dfc-48ac-bdf1-9318e20c111a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load"
    }
   },
   "outputs": [],
   "source": [
    "# Get the latest value of 'created_at' from the control table\n",
    "last_cad_date_control = get_last_cad_date_control()\n",
    "\n",
    "type_of_load = \"incremental\" if last_cad_date_control is not None else \"full\"\n",
    "\n",
    "try:\n",
    "    # If no previous load, perform a full load from bronze table\n",
    "    if last_cad_date_control is None:\n",
    "        print(\"FULL\")\n",
    "        # Read all records from bronze\n",
    "        df_bronze = spark.table(table_cad001p002_bronze)\n",
    "    else:\n",
    "        # Otherwise, perform an incremental load from bronze table\n",
    "        print(\"Incremental\")\n",
    "        # Read only new records from bronze\n",
    "        df_bronze = spark.table(table_cad001p002_bronze).filter(F.col(\"cad_date\") > F.lit(last_cad_date_control))\n",
    "\n",
    "    # Normalize and hash columns for SCD2 comparison\n",
    "    _aux_accents_from = \"áàâãäçéèêëíìîïóòôõöúùûüÁÀÂÃÄÇÉÈÊËÍÌÎÏÓÒÔÕÖÚÙÛÜ\"\n",
    "    _aux_accents_to   = \"aaaaaceeeeiiiiooooouuuuAAAAACEEEEIIIIOOOOOUUUU\"\n",
    "\n",
    "    df_bronze = (\n",
    "        df_bronze\n",
    "            .withColumns({\n",
    "                \"is_active\"    : F.lit(True), # Static value for 'is_active'\n",
    "                \"hash_columns\" : F.sha2(\n",
    "                    F.concat_ws(\"||\",\n",
    "                        F.lower(F.translate(F.regexp_replace(F.col(\"nome\"), \"[^a-zA-Z0-9]\", \"\"),     _aux_accents_from, _aux_accents_to)),\n",
    "                        F.lower(F.translate(F.regexp_replace(F.col(\"endereco\"), \"[^a-zA-Z0-9]\", \"\"), _aux_accents_from, _aux_accents_to)),\n",
    "                        F.col(\"salario\")\n",
    "                    ), 256),\n",
    "                \"_created_at\" : F.current_timestamp(),\n",
    "            })\n",
    "            .dropDuplicates([\"id\", \"hash_columns\"])\n",
    "    )\n",
    "\n",
    "    # data silver path\n",
    "    delta_silver = DeltaTable.forName(spark, table_cad001p002_silver).alias(\"silver\")\n",
    "    \n",
    "    # Insert new records into silver where id does not exist or hash_columns are the same (i.e., new or unchanged data)\n",
    "    delta_silver_insert = delta_silver.merge(\n",
    "        df_bronze.alias(\"bronze\"),\n",
    "        \"silver.id = bronze.id AND silver.hash_columns = bronze.hash_columns\"\n",
    "    ).whenNotMatchedInsert(values={f\"{c}\": f\"bronze.{c}\" for c in df_bronze.columns}).execute()\n",
    "\n",
    "    # Get only the newly inserted records from the silver table\n",
    "    df_full_or_incr = df_bronze\n",
    "\n",
    "    # if incremental, get only the new records affected by silver\n",
    "    if last_cad_date_control is not None:\n",
    "        df_full_or_incr = spark.table(table_cad001p002_silver).filter(F.col(\"cad_date\") > F.lit(last_cad_date_control))\n",
    "\n",
    "    # Create a window specification to partition by 'id' and order by 'id' and 'cad_date' in descending order\n",
    "    # Get the last record for each 'id'\n",
    "    window_spec = Window.partitionBy(\"id\").orderBy(F.col(\"id\"), F.col(\"cad_date\").desc())\n",
    "\n",
    "    # Update existing records in silver table to applay scd 2: \n",
    "    # For each id, if the hash_columns have changed and the new record is more recent (cad_date), \n",
    "    # and the current record has not been updated yet (updated_at IS NULL), \n",
    "    # set is_active to False and updated_at to the new cad_date\n",
    "    delta_silver_update = delta_silver.merge(\n",
    "        df_full_or_incr.withColumn(\"row_number\", F.row_number().over(window_spec)).filter(F.col(\"row_number\") == 1).select(\"id\", \"cad_date\", \"hash_columns\").alias(\"source\"),\n",
    "        \"silver.id = source.id AND silver.hash_columns != source.hash_columns AND silver.cad_date < source.cad_date AND silver.cad_updated IS NULL\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"is_active\" : F.lit(False),\n",
    "        \"cad_updated\": \"source.cad_date\"\n",
    "    }).execute()\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle exceptions and set DataFrame to None\n",
    "    print(str(e)[:300])\n",
    "    df_full_or_incr = None\n",
    "\n",
    "if df_full_or_incr is not None:\n",
    "    # If there are new records, aggregate by 'cad_date' and count rows, then append to control table\n",
    "    (df_full_or_incr\n",
    "        .groupBy(F.col(\"cad_date\"))\n",
    "        .agg(F.count(\"*\").cast(\"int\").alias(\"rows_count\"))\n",
    "        .write.option(\"mergeSchema\", \"true\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(table_cad001p002_control))\n",
    "    \n",
    "    display(\n",
    "        delta_silver_insert\n",
    "            .withColumn(\"operation\", F.lit(\"insert\"))\n",
    "            .unionByName(delta_silver_update.withColumn(\"operation\", F.lit(\"update\")))\n",
    "            .select(*[\"operation\", \"num_affected_rows\", \"num_updated_rows\", \"num_inserted_rows\"])\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6709780930399921,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "p3_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
