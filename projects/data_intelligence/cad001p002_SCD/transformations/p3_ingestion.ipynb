{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23c7dca-1d37-4bed-b18a-4045429d99ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b154b7f-9534-45ea-977e-16914c969cf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Definitions"
    }
   },
   "outputs": [],
   "source": [
    "table_cad001p002_bronze  = \"data_intelligence.bronze.cad001p002\"\n",
    "table_cad001p002_silver  = \"data_intelligence.silver.cad001p002\"\n",
    "table_cad001p002_control = \"data_intelligence.silver.cad001p002_control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd39fbe-a899-423c-9095-a9f5ed17ea2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_last_created_at_control():\n",
    "    return spark.sql(f\"SELECT MAX(created_at) AS last_digtao_ptcao FROM {table_cad001p002_control}\").first()['last_digtao_ptcao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b155d0c-0dfc-48ac-bdf1-9318e20c111a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load"
    }
   },
   "outputs": [],
   "source": [
    "# Get the latest value of 'created_at' from the control table\n",
    "last_created_at_control = get_last_created_at_control()\n",
    "\n",
    "type_of_load = \"incremental\" if last_created_at_control is not None else \"full\"\n",
    "\n",
    "# Define extra columns to add to the DataFrame\n",
    "dict_extra_columns = {\n",
    "    \"is_active\"    : F.lit(True), # Static value for 'is_active'\n",
    "    \"hash_columns\" : F.sha2(F.concat_ws(\"||\", F.col(\"nome\"), F.col(\"endereco\"), F.col(\"salario\")), 256)  # Hash of 'endereco' and 'salario'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # If no previous load, perform a full load from bronze table\n",
    "    if last_created_at_control is None:\n",
    "        print(\"FULL\")\n",
    "        # Read all records from bronze, add extra columns, and remove duplicates by id and hash_columns\n",
    "        df_bronze = spark.table(table_cad001p002_bronze).withColumns(dict_extra_columns).dropDuplicates([\"id\", \"hash_columns\"])\n",
    "    else:\n",
    "        # Otherwise, perform an incremental load from bronze table\n",
    "        print(\"Incremental\")\n",
    "        # Read only new records from bronze (created_at > last_created_at_control), add extra columns, and remove duplicates\n",
    "        df_bronze = (\n",
    "            spark.table(table_cad001p002_bronze)\n",
    "                .filter(F.col(\"created_at\") > F.lit(last_created_at_control))\n",
    "                .withColumns(dict_extra_columns)\n",
    "                .dropDuplicates([\"id\", \"hash_columns\"])\n",
    "        )\n",
    "\n",
    "    # data silver path\n",
    "    delta_silver = DeltaTable.forName(spark, table_cad001p002_silver).alias(\"silver\")\n",
    "    \n",
    "    # Insert new records into silver where id does not exist or hash_columns are the same (i.e., new or unchanged data)\n",
    "    delta_silver_insert = delta_silver.merge(\n",
    "        df_bronze.alias(\"bronze\"),\n",
    "        \"silver.id = bronze.id AND silver.hash_columns = bronze.hash_columns\"\n",
    "    ).whenNotMatchedInsert(values={f\"{c}\": f\"bronze.{c}\" for c in df_bronze.columns}).execute()\n",
    "\n",
    "    # Get only the newly inserted records from the silver table\n",
    "    df_full_or_incr = df_bronze\n",
    "\n",
    "    # if incremental, get only the new records affected by silver\n",
    "    if last_created_at_control is not None:\n",
    "        df_full_or_incr = spark.table(table_cad001p002_silver).filter(F.col(\"created_at\") > F.lit(last_created_at_control))\n",
    "\n",
    "    # Create a window specification to partition by 'id' and order by 'id' and 'created_at' in descending order\n",
    "    # Get the last record for each 'id'\n",
    "    window_spec = Window.partitionBy(\"id\").orderBy(F.col(\"id\"), F.col(\"created_at\").desc())\n",
    "\n",
    "    # Update existing records in silver table to applay scd 2: \n",
    "    # For each id, if the hash_columns have changed and the new record is more recent (created_at), \n",
    "    # and the current record has not been updated yet (updated_at IS NULL), \n",
    "    # set is_active to False and updated_at to the new created_at\n",
    "    delta_silver_update = delta_silver.merge(\n",
    "        df_full_or_incr.withColumn(\"row_number\", F.row_number().over(window_spec)).filter(F.col(\"row_number\") == 1).select(\"id\", \"created_at\", \"hash_columns\", \"row_number\").alias(\"source\"),\n",
    "        \"silver.id = source.id AND silver.hash_columns != source.hash_columns AND silver.created_at < source.created_at AND silver.updated_at IS NULL\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"is_active\" : F.lit(False),\n",
    "        \"updated_at\": \"source.created_at\"\n",
    "    }).execute()\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle exceptions and set DataFrame to None\n",
    "    print(str(e)[:300])\n",
    "    df_full_or_incr = None\n",
    "\n",
    "if df_full_or_incr is not None:\n",
    "    # If there are new records, aggregate by 'created_at' and count rows, then append to control table\n",
    "    (df_full_or_incr\n",
    "        .groupBy(F.col(\"created_at\"))\n",
    "        .agg(F.count(\"*\").cast(\"int\").alias(\"rows_count\"))\n",
    "        .write.option(\"mergeSchema\", \"true\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(table_cad001p002_control))\n",
    "    \n",
    "    display(\n",
    "        delta_silver_insert\n",
    "            .withColumn(\"operation\", F.lit(\"insert\"))\n",
    "            .unionByName(delta_silver_update.withColumn(\"operation\", F.lit(\"update\")))\n",
    "            .select(*[\"operation\", \"num_affected_rows\", \"num_updated_rows\", \"num_inserted_rows\"])\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6709780930399921,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "p3_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
