{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2b64e9-6943-49b1-a353-80e52e5a5ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b722f7-07a8-40cf-b09a-e808da88ca2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class DataLakeConfig:\n",
    "    def __init__(self, layer: str):\n",
    "        self._notebook_context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        self.layer = layer.lower()\n",
    "        self.catalog_name = \"data_intelligence\"\n",
    "        self.schema_name = self._extract_schema_from_path()\n",
    "        self.table_name = \"employees\"\n",
    "        self.workspace_user = self._get_workspace_user()\n",
    "\n",
    "    def _extract_schema_from_path(self) -> str:\n",
    "        notebook_path = self._notebook_context.notebookPath().get()\n",
    "        return Path(notebook_path).name.split(\"_\")[-1].lower()\n",
    "\n",
    "    def _get_workspace_user(self) -> str:\n",
    "        return self._notebook_context.userName().get()\n",
    "\n",
    "    def get_layer_config(self) -> dict:\n",
    "        return {\n",
    "            \"layer\": self.layer,\n",
    "            \"catalog\": self.catalog_name,\n",
    "            \"schema\": self.schema_name,\n",
    "            \"table\": self.table_name,\n",
    "            \"user\": self.workspace_user\n",
    "        }\n",
    "\n",
    "    def get_upstream_layer(self) -> str:\n",
    "        \"\"\"Return the upstream layer that this layer depends on.\"\"\"\n",
    "        if self.layer == \"bronze\":\n",
    "            return \"silver\"\n",
    "        elif self.layer == \"silver\":\n",
    "            return \"gold\"\n",
    "        else:\n",
    "            return None  # Gold usually doesnâ€™t depend on another layer\n",
    "        \n",
    "config = DataLakeConfig(layer=\"bronze\")\n",
    "print(config.get_layer_config())\n",
    "print(\"Upstream Layer:\", config.get_upstream_layer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149921f6-cafb-4490-804c-d9ded10b9df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_CATALOG_NAME:str   = \"data_intelligence\"\n",
    "CONFIG_CATALOG_SCHEMA:str = Path(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()).name.split(\"_\")[-1].lower()\n",
    "CONFIG_CATALOG_TABLE:str  = \"employees\"\n",
    "CONFIG_WORKSPACE_USER:str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abed517a-8556-4c31-8519-54498d858836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if CONFIG_CATALOG_SCHEMA not in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    raise Exception(f\"Invalid schema name: {CONFIG_CATALOG_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc06ea3-a363-4c2a-83a7-dee670263ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def debug_info(msg:str, notice:str = \"Ok\", ljust_len:int = 70) -> None:\n",
    "    print(f\"{msg.ljust(ljust_len, '.')}: {notice}\")\n",
    "\n",
    "def create_delta_table(df:DataFrame, table_name:str = None) -> None:\n",
    "    table_name:str = '.'.join([\n",
    "        CONFIG_CATALOG_SCHEMA,\n",
    "        (CONFIG_CATALOG_TABLE if (table_name is None or len(table_name) == 0) else table_name)\n",
    "    ])\n",
    "    \n",
    "    if spark.catalog.tableExists(table_name):\n",
    "        DeltaTable.forName(spark, table_name).alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            \"target.id = source.id\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ab4a21-b356-4ef5-abf1-89bab87feb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "debug_info(\"var (CONFIG_CATALOG_NAME)\", CONFIG_CATALOG_NAME)\n",
    "debug_info(\"var (CONFIG_CATALOG_SCHEMA)\", CONFIG_CATALOG_SCHEMA)\n",
    "debug_info(\"var (CONFIG_CATALOG_TABLE)\", CONFIG_CATALOG_TABLE)\n",
    "debug_info(\"var (CONFIG_WORKSPACE_USER)\", CONFIG_WORKSPACE_USER)\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CONFIG_CATALOG_NAME}\")\n",
    "debug_info(f\"CREATE CATALOG IF NOT EXISTS {CONFIG_CATALOG_NAME}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CONFIG_CATALOG_NAME}\")\n",
    "debug_info(f\"USE CATALOG {CONFIG_CATALOG_NAME}\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CONFIG_CATALOG_SCHEMA}\")\n",
    "debug_info(f\"CREATE SCHEMA IF NOT EXISTS {CONFIG_CATALOG_SCHEMA}\")\n",
    "\n",
    "debug_info(\"Function -> debug_info\")\n",
    "debug_info(\"Function -> create_delta_table\")\n",
    "\n",
    "if CONFIG_CATALOG_SCHEMA == \"silver\":\n",
    "    if spark.catalog.tableExists(f\"bronze.{CONFIG_CATALOG_TABLE}\"):\n",
    "        debug_info(\"Bronze table exists\")\n",
    "    else:\n",
    "        raise Exception(f\"Bronze table does not exist\")\n",
    "\n",
    "if CONFIG_CATALOG_SCHEMA == \"gold\":\n",
    "    if spark.catalog.tableExists(f\"silver.{CONFIG_CATALOG_TABLE}\"):\n",
    "        debug_info(\"Silver table exists\")\n",
    "    else:\n",
    "        raise Exception(f\"Silver table does not exist\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5907690914035672,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "employees_config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
